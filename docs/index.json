[{"authors":["admin"],"categories":null,"content":"My name is Yuchen Lu. I am currently a P.hD. candidate at the Mila lab of University of Montreal, supervised by Prof. Aaron Courville. Before that I received my undergraduate degree at UIUC working with Prof. Jian Peng. I was also an undergrad at Shanghai Jiao Tong University.\nMy fundamental research interest is language learning as systematic generalization. Humans are able to generate unseen novel utterances from a limited sample of data, while current machine learning approaches fall short on. Building an intelligent agent that is able to acquire the language as efficient as humans is the important next step as we are seeing the marginal effect of increasing model size of language models. I believe there are two main missing pieces of puzzles:\n  Humans learn the language in an embodied environment, and humans acquire language as a tool to influence the world around them. We should model situated language learning beyond learning from a static corpus.\n  Language evolves and adapts to an iterated transmission process so that it becomes structured and easy-to-acquire for the later generations. We should model this cultural evolution aspect of language in our language learning.\n  I enjoying seeing the impact of my research. Recently, our research team, parternered with WebDip successfully developed an AI player for the board game Diplomacy, and it\u0026rsquo;s covered in one of the most popular podcast channels in the community.\nI also co-founded Tuninsight, an award-winning Montreal-based start-up.\nThe email is luyuchen [DOT] paul [AT] gmail [DOT] com.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My name is Yuchen Lu. I am currently a P.hD. candidate at the Mila lab of University of Montreal, supervised by Prof. Aaron Courville. Before that I received my undergraduate degree at UIUC working with Prof. Jian Peng. I was also an undergrad at Shanghai Jiao Tong University.\nMy fundamental research interest is language learning as systematic generalization. Humans are able to generate unseen novel utterances from a limited sample of data, while current machine learning approaches fall short on.","tags":null,"title":"","type":"authors"},{"authors":["Ankit Vani","Max Schwarzer","Yuchen Lu","Eeshan Dhekane","Aaron Courville"],"categories":[],"content":"","date":1611204777,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611204777,"objectID":"b9a95372d6dc9a662adb8bf798c71a7e","permalink":"/publication/ilvqa/","publishdate":"2021-01-20T23:52:57-05:00","relpermalink":"/publication/ilvqa/","section":"publication","summary":"Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.","tags":[],"title":"Iterated learning for emergent systematicity in VQA ","type":"publication"},{"authors":[],"categories":null,"content":"","date":1604902109,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604902109,"objectID":"57933236f9d05aed6f79f00bbdb1eb66","permalink":"/talk/nil_deep_learn/","publishdate":"2020-11-09T01:08:29-05:00","relpermalink":"/talk/nil_deep_learn/","section":"talk","summary":"","tags":[],"title":"Iterated Learning for Deep Learning","type":"talk"},{"authors":["Yuchen Lu","Soumye Singhal","Florian Strub","Olivier Pietquin","Aaron Courville"],"categories":[],"content":"","date":1590295977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590295977,"objectID":"31f49a1e0d71a67b3bf8057f2c9584a6","permalink":"/publication/itlearn/","publishdate":"2020-05-23T23:52:57-05:00","relpermalink":"/publication/itlearn/","section":"publication","summary":"Supervised learning methods excel at capturing statistical properties of language when trained over large text corpora. Yet, these models often produce inconsistent outputs in goal-oriented language settings as they are not trained to completethe underlying task. Moreover, as soon as theagents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we proposea generic approach to counter language drift by using iterated learning. We iterate between finetuning agents with interactive training steps, and periodically replacing them with new agents that are seeded from last iteration and trained to imitate the latest finetuned models. Iterated learning does not require external syntactic constraint nor semantic knowledge, making it a valuable task-agnostic finetuning protocol. We first  explore iterated learning in the Lewis Game. We then scale-up the approach in the translation game. In both settings, our results show that iterated learning drastically counters language drift as well as it improves the task completion metric.","tags":[],"title":"Countering Language Drift with Seeded Iterated Learning","type":"publication"},{"authors":[],"categories":null,"content":"","date":1574316509,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574316509,"objectID":"bb6f2e95ddc9a3a29855ebfc4bc52e69","permalink":"/talk/control_as_inf/","publishdate":"2019-11-21T01:08:29-05:00","relpermalink":"/talk/control_as_inf/","section":"talk","summary":"","tags":[],"title":"Reinforcement Learning and Control as Probabilistic Inference","type":"talk"},{"authors":[],"categories":null,"content":"","date":1568901600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568901600,"objectID":"708010043b55545597c94455251b15f7","permalink":"/talk/evaluate_evaluation/","publishdate":"2019-09-19T14:00:00Z","relpermalink":"/talk/evaluate_evaluation/","section":"talk","summary":"","tags":[],"title":"Paper Presentation: Re-evaluate Evaluation","type":"talk"},{"authors":["Philip Paquette (co-author)","Yuchen Lu (co-author)","Steven Bocco","Max O. Smith","Satya Ortiz-Gagne","Jonathan K. Kummerfeld","Satinder Singh","Joelle Pineau","Aaron Courville"],"categories":[],"content":"","date":1558414377,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558414377,"objectID":"45a09590ee2487229e7fc7d4ea798d37","permalink":"/publication/dipgame/","publishdate":"2019-11-20T23:52:57-05:00","relpermalink":"/publication/dipgame/","section":"publication","summary":"Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents acquire resources through a mix of teamwork and betrayal. Reliance on trust and coordination makes Diplomacy the first non-cooperative multi-agent benchmark for complex sequential social dilemmas in a rich environment. In this work, we focus on training an agent that learns to play the No Press version of Diplomacy where there is no dedicated communication channel between players. We present DipNet, a neural-network-based policy model for No Press Diplomacy. The model was trained on a new dataset of more than 150,000 human games. Our model is trained by supervised learning (SL) from expert trajectories, which is then used to initialize a reinforcement learning (RL) agent trained through self-play. Both the SL and RL agents demonstrate state-of-the-art No Press performance by beating popular rule-based bots.","tags":[],"title":"No Press Diplomacy: Modeling Multi-Agent Gameplay","type":"publication"},{"authors":[],"categories":[],"content":"What is Label Leaking Effect? Label leaking effect, is the phenomenon that during adversarial training, the validation error on adversarial examples are smaller than the validation error on clean examples. The effect is reported in this paper, and it is an interesting one. Adversarial examples are supposed to confuse the neural nets during training so as to increase the robustness. It is like a form of data augmentation, and it is supposed to increase the difficulty for neural nets to success. Nevertheless, at the end of training, models are able to solve the problem of classifying adversarial examples better than the original problem.\nThe problem is that when one uses one-step methods that involves the true labels to craft adversarial samples, there is extra information about label being leaked, thus making the problem simple. To see how it works, let\u0026rsquo;s start by examine one such method, Fast Gradient Sign Method (FGSM).\nHow Does FGSM Leak Label Information? Suppose we have original data $X,Y_{true}$ and a model $F$ . Our prediction is $Y_{pred} = F(X)$. Our loss function is $L(Y_{true}, Y_{pred})$ telling us how far $Y_{pred}$ is away from $Y_{true}$. In FGSM, the motivation is to increase the loss by add to $X$ a small perturbation which is proportional to the sign of the gradient of $L$ w.r.t. $X$. That is $$ X_{adv} = X + \\epsilon sign( \\frac{\\partial L(Y_{true}, Y_{pred})}{\\partial X} ) $$ where $X_{adv}$ is called the adversarial sample, and $\\epsilon$ controls the step size. In the case of image-classification, we could further add some details. That is $$ output = f(X) $$ $$ Y_{pred} = SoftMax(output) $$ $$ L(Y_{true}, Y_{pred}) = CrossEntropy(Y_{true}, Y_{pred}) $$\nThen we have $$ \\frac{\\partial L(Y_{true}, Y_{pred})}{\\partial X}=\\frac{\\partial L(Y_{true}, Y_{pred})}{\\partial output} \\frac{\\partial output}{\\partial X}=(Y_{pred}-Y_{true})\\frac{\\partial output}{\\partial X} $$ For different $X$, the part of $\\frac{\\partial output}{\\partial X}$ is dominated by a big product of weight matrix and activations. For two images with the same label, their activations in any fixed networks are close, and the weights of the network are fixed. As a result $\\frac{\\partial output}{\\partial X}$ is almost a constant for $X$ with the same label. That means, the resulting gradient is highly correlated with the $Y_{true}$, and therefore the label information is leaked in this gradient term.\nAs a result, when added the gradient, classifying $X_{adv}$ could become a simpler problem than the original problem of classifying $X$, as $X_{adv}$ contains extra information from the added gradient.\nI did an experiment on MNIST. The original model is a three layer MLP with random initialization. After extract the sign of the gradients, I further transform each gradient between 0 and 1 by $$ X_{noise} = clip(gradient+1, 0, 1) $$ I fit a simple linear model on the dataset $X_{noise}, Y_{true}$ using SGD without any careful tuning. As a result, I obtained a $96%$ validation error. This confirms the hypothesis.\nIf classifying $X_{adv}$ becomes a easier problem, then one can imagine that when doing training by adding adversarial samples, the network would find a weight configuration such that it fully take advantage of the extra label information hidden in the label. If the difficulty gap between the original $X$ and $X_{adv}$ is too large, then it is expected that the validation error on the adversarial samples are lower. That\u0026rsquo;s why we don\u0026rsquo;t observe label leaking effect on MNIST but only on ImageNet, since classifying $X$ is easy enough.\nLabel Leaking in Cifar-10 To further illustrate this effect, I use FGSM to adversarial train a resNet with 110 layers on Cifar10, since no one has tried to reproduce this effect on cifar yet. The adversarial training process is the same as here. I replaced half of the batch with adversarial samples, with $\\epsilon \\sim N(\\mu=0, \\sigma=12)$, and I discarded the negative $\\epsilon$ sampled. During evaluation, I set $\\epsilon$ to be 6. After carefully tuning the learning rate and momentum, I obtain validation accuracy on clean and adversarial examples to be respectively $92.12%$ and $94.81%$. Boom, mission completed.\nIn order to further appreciate this effect, I also provide the cross entropy vs $\\epsilon$ graph on the validation data. By varying the $\\epsilon$ along the direction along which the loss increases fastest, we could find some interesting insight about why adversarial training work.\n  Plot of cross entropy vs. $\\epsilon$ in FGSM    With increasing $\\epsilon$, the loss first increases. This is understandable since gradient is the direction along which the loss increases fastest. After a peak, the loss start decreasing. This is the gradient masking effect of adversarial training, as is also reported in Figure 2 of this paper. We can also see at $\\epsilon=6$, the loss is even lower than $\\epsilon=0$, which is consistent with the label leaking effect we observed before. This is graph also illustrate why adversarial training using one-step FGSM cannot defend attacks from iterative FGSM. In iterative methods, the attacker could still follow the curve and find that little peak to fool the network.  Conclusion Label leaking effect is an counter-intuitive phenomenon, since we are expecting that adversarial samples are a form of data augmentation, and thus are harder problems to solve. After these experiments, I believe this effect is caused by the correlation between the added noise and true label, and this effect would occur only when the original classification problem is very hard.\nOne more question is if we can avoid this by using least-likely class when calculating the gradients, I personally don\u0026rsquo;t think that it can solve the problem. Because even if we are not using the true label, for images in the same class, they might have the same least-likely class label, because of the same reason that they have similar activations. As a result, there is still a strong correlation between the gradient and the true label, because of the strong correlation between the least-likely label and the true label. Nevertheless, further experiment could be done to prove or disprove this.\n","date":1497830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497830400,"objectID":"a42a183b4c52538dd5428ff94305ab20","permalink":"/post/label_leak/","publishdate":"2017-06-19T00:00:00Z","relpermalink":"/post/label_leak/","section":"post","summary":"A strange phenomenon during adversarial training","tags":[],"title":"Label Leaking in Adversarial Training","type":"post"}]